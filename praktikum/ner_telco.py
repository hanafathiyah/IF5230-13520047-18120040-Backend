# -*- coding: utf-8 -*-
"""NER Telco_13520047_18120040.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15albtad6Olvnem9Pzd8t0nNfovI45eGy
"""

!pip install accelerate datasets evaluate seqeval

"""# LSTM"""

from datasets import load_dataset

cyber = load_dataset("Gizachew/cybersecurity-ner")

cyber["train"][0]

label_list = cyber["train"].features[f'ner_tags'].feature.names
label_list

class Tokenizer:
  def __init__(self, lower=True):
    self.token2id = dict()
    self.id2token = dict()
    self.n_vocab = 0
    self.lower = lower

  def add_token(self, token):
    self.token2id[token] = self.n_vocab
    self.id2token[self.n_vocab] = token
    self.n_vocab += 1

  def fit(self, texts, oov=True):
    if "<OOV>" not in self.token2id and oov:
      self.add_token("<OOV>") # OOV
    if "<PAD>" not in self.token2id:
      self.add_token("<PAD>") # PADDING

    for text in texts:
      if isinstance(text, str):
        text = text.split()
      for t in text:
        if self.lower:
          t = t.lower()
        if t not in self.token2id:
          self.add_token(t)
    return self

  def transform(self, texts, padding="right"):
    res = []
    max_len = 0
    for text in texts:
      if isinstance(text, str):
        text = text.split()

      if len(text) > max_len:
        max_len = len(text)

      entry = []
      for t in text:
        if self.lower:
          t = t.lower()
        entry.append(
            self.token2id[t] if t in self.token2id else self.token2id["<OOV>"]
        )
      res.append(entry)

    for i in range(len(res)):
      pad = [self.token2id["<PAD>"] for _ in range(max_len - len(res[i]))]
      res[i] = res[i] + pad if padding == "right" else pad + res[i]
    return res

tokenizer = Tokenizer(lower=True).fit(cyber["train"]["tokens"])

tokenizer.n_vocab

cyber["train"]["tokens"][:3]

tokenizer.transform(cyber["train"]["tokens"][:3])

label_encoder = Tokenizer(lower=False)
label_encoder.token2id = {
    el : (i + 1) for i, el in enumerate(label_list)
}
label_encoder.token2id["<PAD>"] = 0

label_encoder.id2token = {v : k for k, v in label_encoder.token2id.items()}
label_encoder.n_vocab = len(label_list) + 1

import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np

X_train = np.array(tokenizer.transform(cyber["train"]["tokens"], padding="right"))
y_train = np.array(label_encoder.transform(list(map(lambda x: [label_list[i] for i in x], cyber["train"]["ner_tags"])), padding="right"))

X_val = np.array(tokenizer.transform(cyber["validation"]["tokens"], padding="right"))
y_val = np.array(label_encoder.transform(list(map(lambda x: [label_list[i] for i in x], cyber["validation"]["ner_tags"])), padding="right"))

X_test = np.array(tokenizer.transform(cyber["test"]["tokens"], padding="right"))
y_test = np.array(label_encoder.transform(list(map(lambda x: [label_list[i] for i in x], cyber["test"]["ner_tags"])), padding="right"))

vocab_size = tokenizer.n_vocab
embedding_dim = 64
hidden_dim = 64
num_classes = label_encoder.n_vocab

# Model
inputs = layers.Input(shape=(None,))
embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)
lstm_layer = layers.LSTM(units=hidden_dim, return_sequences=True)(embedding_layer)
outputs = layers.TimeDistributed(layers.Dense(num_classes, activation='softmax'))(lstm_layer)

model = Model(inputs=inputs, outputs=outputs)
model.summary()

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

with tf.device('/GPU:0'):
    model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val))

pred = model(X_test).numpy().argmax(-1)

pred

pred = [
    [label_encoder.id2token[i] for i in t] for t in pred
]

y_true = [
    [label_encoder.id2token[i] for i in t] for t in y_test
]

pred[0]

import evaluate

seqeval = evaluate.load("seqeval")

score = seqeval.compute(predictions=pred, references=y_true)

score

"""# Huggingface Training

Referensi: https://huggingface.co/docs/transformers/en/tasks/token_classification
"""

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

example = cyber["train"][0]
tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
tokens

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples[f"ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:  # Set the special tokens to -100.
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:  # Only label the first token of a given word.
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_cyber = cyber.map(tokenize_and_align_labels, batched=True)

from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

import numpy as np

labels = [label_list[i] for i in example[f"ner_tags"]]


def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

id2label = {
    0: "B-Indicator",
    1: "B-Malware",
    2: "B-Organization",
    3: "B-System",
    4: "B-Vulnerability",
    5: "I-Indicator",
    6: "I-Malware",
    7: "I-Organization",
    8: "I-System",
    9: "I-Vulnerability",
    10: "O",
}
label2id = {
    "B-Indicator": 0,
    "B-Malware": 1,
    "B-Organization": 2,
    "B-System": 3,
    "B-Vulnerability": 4,
    "I-Indicator": 5,
    "I-Malware": 6,
    "I-Organization": 7,
    "I-System": 8,
    "I-Vulnerability": 9,
    "O": 10
}

from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained(
    "distilbert/distilbert-base-uncased", num_labels=11, id2label=id2label, label2id=label2id
)

training_args = TrainingArguments(
    output_dir="my_awesome_cyber_model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    # push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_cyber["train"],
    eval_dataset=tokenized_cyber["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

text = "A few days back, we wrote about an Android Marcher trojan variant posing as the Super Mario game for Android"

from transformers import pipeline

classifier = pipeline("ner", model=model, tokenizer=tokenizer)
classifier(text)

"""# Huggingface Pipeline"""

classifier = pipeline("ner", model="dslim/bert-base-NER")
classifier(text)

